# DRW-Crypto-Market-Prediction
This project is based on the DRW Crypto Market Prediction Kaggle competition. The data and competition details can be found [here](https://www.kaggle.com/competitions/drw-crypto-market-prediction/overview).

The dataset contains order book information along with 780 masked market features. In the [feature_engineering](https://github.com/mahsa-khoshnama/DRW-Crypto-Market-Prediction/blob/main/code/feature_engineering.ipynb) notebook, I show that these features are highly correlated, making it suboptimal to incorporate them directly into prediction models. To address this, I apply hierarchical clustering to group the features into 174 clusters, which are then used in two different prediction models.

Because crypto market data is very noisy, both models rely on ensembling multiple boosted trees. Training many models on different slices of the data or on different feature subsets helps reduce variance and makes the final predictions more stable and less sensitive to noise. This improves generalization compared to relying on a single model. To further guard against overfitting in this noisy setting, I restrict the hyperparameter ranges (e.g., using lower bagging and feature fractions and limiting the number of boosting rounds) so that the models stay relatively simple and generalize better.

Note that the competition data was revised several times. In the final release, the timestamps in the test set were removed, which prevented the use of models incorporating lagged features as inputs. As a result, I rely only on the time-series structure of the prediction problem when constructing the trainâ€“validation split: the earliest 80% of the data is used for training, and the most recent 20% is used for validation. This temporal structure is also leveraged to further slice the training and validation sets into multiple subsamples, which allows models to be trained on diverse segments of the data. This subsampling strategy reduces variance, improves robustness, and makes the ensemble less sensitive to local noise patterns in the data.

In the first model, I average the masked features within each cluster and use these aggregated cluster features, together with the order book data, to train 20 boosted trees on 20 sliced subsamples of the data. Their predictions are then combined using a ridge regression. Full details of this model are provided in the lgbm_stack_ensemble_cluster_means notebook.

In the second model, I generate feature subsamples by sampling from clusters such that no two features from the same cluster appear in the same subset. Multiple boosted trees are then trained on time-sliced data, this time divided into 5 subsamples, with each tree trained on a different sampled feature subset. In total, this model produces 60 boosted trees, whose predictions are combined through a ridge regression. This approach achieves better validation performance compared to the first baseline model. Full details of this model are provided in the lgbm_stack_ensemble_cluster_sampling notebook.
