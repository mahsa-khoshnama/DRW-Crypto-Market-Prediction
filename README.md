# DRW-Crypto-Market-Prediction
This project is based on the DRW Crypto Market Prediction Kaggle competition. The data and competition details can be found [here](https://www.kaggle.com/competitions/drw-crypto-market-prediction/overview).

The dataset contains order book information along with 780 masked market features. In the feature_engineering notebook, I show that these features are highly correlated, making it suboptimal to incorporate them directly into prediction models. To address this, I apply hierarchical clustering to group the features into 174 clusters, which are then used in two different prediction models.

Given the high level of noise inherent in crypto market data, both models adopt an ensemble-based design using multiple boosted trees. Ensembling mitigates the variance of individual learners by aggregating across many models trained on different data slices or feature subsets. This reduces sensitivity to noise, stabilizes predictions, and improves out-of-sample generalization compared to a single model. In other words, the ensemble leverages the bias–variance tradeoff to extract more reliable signals from noisy financial features. To further guard against overfitting in this noisy setting, I constrain the search space for model hyperparameters (e.g., lower ranges for bagging and feature fractions, and fewer boosted trees), encouraging simpler models that generalize better.

Note that the competition data was revised several times. In the final release, the timestamps in the test set were removed, which prevented the use of models incorporating lagged features as inputs. As a result, I rely only on the time-series structure of the prediction problem when constructing the train–validation split: the earliest 80% of the data is used for training, and the most recent 20% is used for validation. This temporal structure is also leveraged to further slice the training and validation sets into multiple subsamples, which allows models to be trained on diverse segments of the data. This subsampling strategy reduces variance, improves robustness, and makes the ensemble less sensitive to local noise patterns in the data.

In the first model, I average the masked features within each cluster and use these aggregated cluster features, together with the order book data, to train 20 boosted trees on 20 sliced subsamples of the data. Their predictions are then combined using a ridge regression. Full details of this model are provided in the lgbm_stack_ensemble_cluster_means notebook.

In the second model, I generate feature subsamples by sampling from clusters such that no two features from the same cluster appear in the same subset. Multiple boosted trees are then trained on time-sliced data, this time divided into 5 subsamples, with each tree trained on a different sampled feature subset. In total, this model produces 60 boosted trees, whose predictions are combined through a ridge regression. This approach achieves better validation performance compared to the first baseline model. Full details of this model are provided in the lgbm_stack_ensemble_cluster_sampling notebook.
