{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa77b585-f746-4524-8743-395eb92721cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries and settings\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbaa51b8-91bb-47bc-a049-1e5d1f146448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('../data/input/train.parquet')\n",
    "df_test = pd.read_parquet('../data/input/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70504697-609e-4daa-947a-e97bcfd77a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering the features\n",
    "X_cols = df_train.columns[df_train.columns.str.startswith('X')]\n",
    "col2cluster = pd.Series(pd.read_csv('../data/intermediate/clusters.csv', index_col = 0).iloc[:,0])\n",
    "\n",
    "# Group columns by cluster and take the mean across features inside each cluster\n",
    "df_reduced = (\n",
    "    df_train[X_cols].groupby(col2cluster, axis=1)\n",
    "      .mean()\n",
    "    #   .sort_index(axis=1) \n",
    ")\n",
    "\n",
    "df_reduced.set_index(df_train.index, inplace = True)\n",
    "df_reduced = pd.merge(\n",
    "    df_train[['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label']],\n",
    "    df_reduced,\n",
    "    left_index = True, \n",
    "    right_index = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_reduced_test = (\n",
    "    df_test[X_cols].groupby(col2cluster, axis=1)\n",
    "      .mean()\n",
    "    #   .sort_index(axis=1) \n",
    ")\n",
    "\n",
    "df_reduced_test.set_index(df_test.index, inplace = True)\n",
    "df_reduced_test = pd.merge(\n",
    "    df_test[['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']],\n",
    "    df_reduced_test,\n",
    "    left_index = True, \n",
    "    right_index = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031ded28-f77d-4b0b-b231-d2b18465a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" **************** ENSEMBLE MODEL ****************** \"\"\"\n",
    "class LightGBMTimeSeriesEnsemble:\n",
    "    def __init__(self, n_models, n_trials, alphas=None, metric='rmse', random_seed=42):\n",
    "        \"\"\"\n",
    "        n_models  : number of base learners (e.g. 50)\n",
    "        n_trials  : Optuna trials per learner\n",
    "        alphas    : candidate ridge regularization strengths\n",
    "        metric    : LightGBM eval metric (for early stopping)\n",
    "        \"\"\"\n",
    "        self.n_models = n_models\n",
    "        self.n_trials = n_trials\n",
    "        self.metric = metric\n",
    "        self.alphas = alphas if alphas is not None else np.logspace(-2, 5, 30)\n",
    "        self.seed = random_seed\n",
    "\n",
    "        self.best_params_list = []\n",
    "        self.best_iterations = []\n",
    "        self.models = []\n",
    "        self.ridge = None\n",
    "        self.best_alpha = None\n",
    "\n",
    "    def _optuna_objective(self, trial, X_tr, y_tr, X_val, y_val):\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',  \n",
    "            'verbosity': -1,\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 16, 64),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 0.5),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.6),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
    "            'num_boost_round': trial.suggest_int('num_boost_round', 200,500),\n",
    "            'early_stopping_rounds': 50\n",
    "        }\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgb.Dataset(X_tr, y_tr),\n",
    "            valid_sets=[lgb.Dataset(X_val, y_val)]\n",
    "        )\n",
    "        pred = model.predict(X_val)\n",
    "        corr, _ = pearsonr(pred, y_val)\n",
    "        return -corr\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        1) Fit each base model on X_train[i::n] versus y_train[i::n] with Optuna tuning.\n",
    "        Next steps are carried out in the fit_ridge function:\n",
    "        2) Make predictions of each base model on *full* X_train => (n_samples_train x n_models)\n",
    "        3) Ridge-fit on (train_preds, y_train)\n",
    "        4) Select alpha by evaluating ridge on full X_val predictions\n",
    "        \"\"\"\n",
    "        self.models = []\n",
    "        self.best_params_list = []\n",
    "        self.best_iterations = []\n",
    "\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            print(f'Training model {i+1}/{self.n_models}')\n",
    "            Xt = X_train.iloc[i::self.n_models]\n",
    "            yt = y_train.iloc[i::self.n_models]\n",
    "            Xv = X_val.iloc[i::self.n_models]\n",
    "            yv = y_val.iloc[i::self.n_models]\n",
    "\n",
    "            # Tune\n",
    "            sampler = optuna.samplers.TPESampler(seed=self.seed)\n",
    "            study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "            study.optimize(\n",
    "                lambda trial: self._optuna_objective(trial, Xt, yt, Xv, yv),\n",
    "                n_trials=self.n_trials,\n",
    "            )\n",
    "            best_params = study.best_params\n",
    "            best_params['seed'] = self.seed\n",
    "            \n",
    "\n",
    "            # Train model with best params\n",
    "            model = lgb.train(\n",
    "                {\n",
    "                **best_params, \n",
    "                'objective':'regression',\n",
    "                'metric':self.metric,\n",
    "                'verbosity':-1, \n",
    "                'early_stopping_rounds': 50\n",
    "                },\n",
    "                lgb.Dataset(Xt, yt),\n",
    "                valid_sets=[lgb.Dataset(Xv, yv)]\n",
    "            )\n",
    "            best_params['num_boost_round'] = model.best_iteration\n",
    "            self.best_params_list.append(best_params)\n",
    "            self.best_iterations.append(model.best_iteration)\n",
    "            self.models.append(model)\n",
    "\n",
    "        self.fit_ridge(X_train, y_train, X_val, y_val, self.alphas)\n",
    "        \n",
    "        \n",
    "    def fit_ridge(self, X_train, y_train, X_val, y_val, alphas):\n",
    "        \"\"\"\n",
    "        1) Make predictions of each base model on *full* X_train => (n_samples_train x n_models)\n",
    "        2) Ridge-fit on (train_preds, y_train)\n",
    "        3) Select alpha by evaluating ridge on full X_val predictions\n",
    "        \"\"\"\n",
    "        # --- predictions on full training set ---\n",
    "        train_stack = np.column_stack([m.predict(X_train, num_iteration=it)\n",
    "                                       for m, it in zip(self.models, self.best_iterations)])\n",
    "\n",
    "        # --- search alpha by evaluating on validation set ---\n",
    "        val_stack = np.column_stack([m.predict(X_val, num_iteration=it)\n",
    "                                     for m, it in zip(self.models, self.best_iterations)])\n",
    "\n",
    "        self.alphas = alphas\n",
    "        best_corr = -np.inf\n",
    "        for a in self.alphas:\n",
    "            r = Ridge(alpha=a)\n",
    "            r.fit(train_stack, y_train)\n",
    "            corr, _ = pearsonr(r.predict(val_stack), y_val)\n",
    "            print(f' ridge alpha: {a:.5f}, val corr: {corr:.5f}')\n",
    "            if corr > best_corr:\n",
    "                best_corr = corr\n",
    "                self.best_alpha = a\n",
    "                self.ridge = r\n",
    "\n",
    "        print(f'Best ridge alpha: {self.best_alpha:.5f}, val corr: {best_corr:.5f}')\n",
    "    \n",
    "    \n",
    "    def refit_full(self, X_full, y_full):\n",
    "        \"\"\"\n",
    "        Retrain base models and the ridge regression model on full data using best params and alpha.\n",
    "        \"\"\"\n",
    "        self.models = []\n",
    "        for i, params in enumerate(self.best_params_list):\n",
    "            print(f'Refitting model {i+1}/{self.n_models} on full data...')\n",
    "            model = lgb.train(\n",
    "                {\n",
    "                **params,  \n",
    "                'objective':'regression',\n",
    "                'metric':self.metric,\n",
    "                'verbosity':-1\n",
    "                },\n",
    "                lgb.Dataset(X_full, y_full)\n",
    "            )\n",
    "            self.models.append(model)\n",
    "        \n",
    "        full_stack = np.column_stack([\n",
    "            m.predict(X_full, num_iteration=it) for m, it in zip(self.models, self.best_iterations)\n",
    "        ])\n",
    "        self.ridge = Ridge(alpha=self.best_alpha)\n",
    "        self.ridge.fit(full_stack, y_full)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict stacked output on full dataset X\n",
    "        \"\"\"\n",
    "        base = np.column_stack([m.predict(X, num_iteration=it)\n",
    "                                for m, it in zip(self.models, self.best_iterations)])\n",
    "        return self.ridge.predict(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e6c44-ef97-4f41-ba4c-f5127b9ddbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-19 15:00:03,975] A new study created in memory with name: no-name-24eaec7b-df24-4606-88fd-532dc2cfc34c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-19 15:00:04,756] Trial 0 finished with value: -0.08720271523373274 and parameters: {'learning_rate': 0.015355286838886862, 'num_leaves': 62, 'max_depth': 5, 'min_data_in_leaf': 64, 'feature_fraction': 0.1624074561769746, 'bagging_fraction': 0.26239780813448105, 'bagging_freq': 1, 'lambda_l1': 4.330880728874676, 'lambda_l2': 3.005575058716044, 'num_boost_round': 413}. Best is trial 0 with value: -0.08720271523373274.\n",
      "[I 2025-08-19 15:00:06,343] Trial 1 finished with value: -0.10092598658873288 and parameters: {'learning_rate': 0.005318033256270142, 'num_leaves': 63, 'max_depth': 6, 'min_data_in_leaf': 29, 'feature_fraction': 0.17272998688284025, 'bagging_fraction': 0.27336180394137355, 'bagging_freq': 4, 'lambda_l1': 2.6237821581611893, 'lambda_l2': 2.1597250932105787, 'num_boost_round': 287}. Best is trial 1 with value: -0.10092598658873288.\n",
      "[I 2025-08-19 15:00:07,118] Trial 2 finished with value: -0.09174133779910788 and parameters: {'learning_rate': 0.03126143958203108, 'num_leaves': 22, 'max_depth': 3, 'min_data_in_leaf': 43, 'feature_fraction': 0.28242799368681437, 'bagging_fraction': 0.5140703845572054, 'bagging_freq': 2, 'lambda_l1': 2.571172192068058, 'lambda_l2': 2.9620728443102124, 'num_boost_round': 213}. Best is trial 1 with value: -0.10092598658873288.\n",
      "[I 2025-08-19 15:00:07,941] Trial 3 finished with value: -0.1125088008527004 and parameters: {'learning_rate': 0.030860579740535368, 'num_leaves': 24, 'max_depth': 2, 'min_data_in_leaf': 96, 'feature_fraction': 0.4862528132298237, 'bagging_fraction': 0.5233589392465845, 'bagging_freq': 4, 'lambda_l1': 0.48836057003191935, 'lambda_l2': 3.4211651325607844, 'num_boost_round': 332}. Best is trial 3 with value: -0.1125088008527004.\n",
      "[I 2025-08-19 15:00:08,810] Trial 4 finished with value: -0.09866692694667052 and parameters: {'learning_rate': 0.007206848764305203, 'num_leaves': 40, 'max_depth': 2, 'min_data_in_leaf': 92, 'feature_fraction': 0.20351199264000677, 'bagging_fraction': 0.46500891374159276, 'bagging_freq': 4, 'lambda_l1': 2.600340105889054, 'lambda_l2': 2.7335513967163982, 'num_boost_round': 255}. Best is trial 3 with value: -0.1125088008527004.\n",
      "[I 2025-08-19 15:00:09,765] Trial 5 finished with value: -0.10821011169791131 and parameters: {'learning_rate': 0.09129115219600122, 'num_leaves': 53, 'max_depth': 6, 'min_data_in_leaf': 91, 'feature_fraction': 0.3391599915244341, 'bagging_fraction': 0.5687496940092467, 'bagging_freq': 1, 'lambda_l1': 0.979914312095726, 'lambda_l2': 0.22613644455269033, 'num_boost_round': 297}. Best is trial 3 with value: -0.1125088008527004.\n",
      "[I 2025-08-19 15:00:10,824] Trial 6 finished with value: -0.13338677343765942 and parameters: {'learning_rate': 0.016019568611465234, 'num_leaves': 29, 'max_depth': 6, 'min_data_in_leaf': 42, 'feature_fraction': 0.2123738038749523, 'bagging_fraction': 0.4170784332632994, 'bagging_freq': 2, 'lambda_l1': 4.010984903770199, 'lambda_l2': 0.3727532183988541, 'num_boost_round': 497}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:11,692] Trial 7 finished with value: -0.0907808586432367 and parameters: {'learning_rate': 0.05054562991054155, 'num_leaves': 25, 'max_depth': 2, 'min_data_in_leaf': 84, 'feature_fraction': 0.38274293753904687, 'bagging_fraction': 0.4916028672163949, 'bagging_freq': 8, 'lambda_l1': 0.3702232586704518, 'lambda_l2': 1.7923286427213632, 'num_boost_round': 234}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:12,343] Trial 8 finished with value: -0.12627908450503253 and parameters: {'learning_rate': 0.06635802485202448, 'num_leaves': 46, 'max_depth': 3, 'min_data_in_leaf': 15, 'feature_fraction': 0.2243929286862649, 'bagging_fraction': 0.33007332881069884, 'bagging_freq': 8, 'lambda_l1': 3.1877873567760657, 'lambda_l2': 4.436063712881633, 'num_boost_round': 342}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:13,802] Trial 9 finished with value: -0.10780978338772751 and parameters: {'learning_rate': 0.007154276249073926, 'num_leaves': 50, 'max_depth': 5, 'min_data_in_leaf': 61, 'feature_fraction': 0.40838687198182444, 'bagging_fraction': 0.3975182385457563, 'bagging_freq': 6, 'lambda_l1': 2.137705091792748, 'lambda_l2': 0.12709563372047594, 'num_boost_round': 232}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:15,130] Trial 10 finished with value: -0.09549265587511294 and parameters: {'learning_rate': 0.01576783110186712, 'num_leaves': 33, 'max_depth': 5, 'min_data_in_leaf': 41, 'feature_fraction': 0.1088523500759235, 'bagging_fraction': 0.40155513807104737, 'bagging_freq': 10, 'lambda_l1': 4.8648127756677635, 'lambda_l2': 1.0512396178898649, 'num_boost_round': 498}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:16,073] Trial 11 finished with value: -0.0945353427830749 and parameters: {'learning_rate': 0.09766018896330539, 'num_leaves': 37, 'max_depth': 4, 'min_data_in_leaf': 13, 'feature_fraction': 0.25605815620957617, 'bagging_fraction': 0.349421748581656, 'bagging_freq': 7, 'lambda_l1': 3.8294445036020566, 'lambda_l2': 4.689181419475894, 'num_boost_round': 394}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:17,361] Trial 12 finished with value: -0.1081980974040271 and parameters: {'learning_rate': 0.015314041101629105, 'num_leaves': 49, 'max_depth': 3, 'min_data_in_leaf': 17, 'feature_fraction': 0.23905337219863323, 'bagging_fraction': 0.3298299858594789, 'bagging_freq': 9, 'lambda_l1': 3.5598890110670585, 'lambda_l2': 4.913117318067797, 'num_boost_round': 498}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:18,208] Trial 13 finished with value: -0.10186267328660456 and parameters: {'learning_rate': 0.05491528432184406, 'num_leaves': 29, 'max_depth': 4, 'min_data_in_leaf': 28, 'feature_fraction': 0.1284805429346761, 'bagging_fraction': 0.4169803526668513, 'bagging_freq': 6, 'lambda_l1': 3.3571368706434783, 'lambda_l2': 3.9807574882147962, 'num_boost_round': 424}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:18,886] Trial 14 finished with value: -0.11462945472970254 and parameters: {'learning_rate': 0.02631244922019963, 'num_leaves': 17, 'max_depth': 3, 'min_data_in_leaf': 47, 'feature_fraction': 0.3222019005320945, 'bagging_fraction': 0.21953224586368963, 'bagging_freq': 8, 'lambda_l1': 1.439546497325321, 'lambda_l2': 1.3214014459807881, 'num_boost_round': 361}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:19,710] Trial 15 finished with value: -0.10021586160994382 and parameters: {'learning_rate': 0.04946183481414609, 'num_leaves': 45, 'max_depth': 4, 'min_data_in_leaf': 75, 'feature_fraction': 0.21409507130737004, 'bagging_fraction': 0.34539174351144564, 'bagging_freq': 3, 'lambda_l1': 4.898232498406145, 'lambda_l2': 3.8187369745742643, 'num_boost_round': 458}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:21,186] Trial 16 finished with value: -0.09138844527626734 and parameters: {'learning_rate': 0.010144724046185803, 'num_leaves': 34, 'max_depth': 6, 'min_data_in_leaf': 29, 'feature_fraction': 0.2823940025707348, 'bagging_fraction': 0.45013812181030344, 'bagging_freq': 10, 'lambda_l1': 3.132394567758082, 'lambda_l2': 1.0500119716802696, 'num_boost_round': 352}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:22,225] Trial 17 finished with value: -0.12043524027427896 and parameters: {'learning_rate': 0.018629771817491772, 'num_leaves': 44, 'max_depth': 3, 'min_data_in_leaf': 22, 'feature_fraction': 0.17332214232563045, 'bagging_fraction': 0.2939456118049576, 'bagging_freq': 5, 'lambda_l1': 4.134336554829187, 'lambda_l2': 4.093784190410645, 'num_boost_round': 316}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:23,387] Trial 18 finished with value: -0.10874322421236397 and parameters: {'learning_rate': 0.011489992808860986, 'num_leaves': 55, 'max_depth': 5, 'min_data_in_leaf': 54, 'feature_fraction': 0.22041219733010692, 'bagging_fraction': 0.36408816903572394, 'bagging_freq': 7, 'lambda_l1': 1.620127027675733, 'lambda_l2': 2.0907834521818196, 'num_boost_round': 385}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:24,002] Trial 19 finished with value: -0.10158633420522138 and parameters: {'learning_rate': 0.06565789104850346, 'num_leaves': 41, 'max_depth': 3, 'min_data_in_leaf': 10, 'feature_fraction': 0.3651250666290948, 'bagging_fraction': 0.3087453034066942, 'bagging_freq': 2, 'lambda_l1': 4.365712142609362, 'lambda_l2': 0.5854117658759617, 'num_boost_round': 461}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:24,791] Trial 20 finished with value: -0.09237475194198183 and parameters: {'learning_rate': 0.03887497205274868, 'num_leaves': 31, 'max_depth': 4, 'min_data_in_leaf': 32, 'feature_fraction': 0.44024685305652084, 'bagging_fraction': 0.20606690198827948, 'bagging_freq': 8, 'lambda_l1': 2.862350728676124, 'lambda_l2': 4.388518995628798, 'num_boost_round': 456}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:25,439] Trial 21 finished with value: -0.10953871722502717 and parameters: {'learning_rate': 0.01937045631965136, 'num_leaves': 44, 'max_depth': 3, 'min_data_in_leaf': 22, 'feature_fraction': 0.1654937608782899, 'bagging_fraction': 0.291872769103278, 'bagging_freq': 5, 'lambda_l1': 4.011364372839362, 'lambda_l2': 3.880772721674152, 'num_boost_round': 310}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:26,263] Trial 22 finished with value: -0.10217964721720947 and parameters: {'learning_rate': 0.020880953024184665, 'num_leaves': 46, 'max_depth': 3, 'min_data_in_leaf': 37, 'feature_fraction': 0.2587597002946329, 'bagging_fraction': 0.2448450201170167, 'bagging_freq': 5, 'lambda_l1': 4.354258188317525, 'lambda_l2': 4.3018145485558925, 'num_boost_round': 324}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:27,168] Trial 23 finished with value: -0.10404330027485405 and parameters: {'learning_rate': 0.01139037692800468, 'num_leaves': 37, 'max_depth': 2, 'min_data_in_leaf': 22, 'feature_fraction': 0.19799320923374086, 'bagging_fraction': 0.37188560514055224, 'bagging_freq': 3, 'lambda_l1': 3.72143769600543, 'lambda_l2': 3.3960557771380357, 'num_boost_round': 276}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:28,185] Trial 24 finished with value: -0.11128652127518865 and parameters: {'learning_rate': 0.025756633169507118, 'num_leaves': 53, 'max_depth': 4, 'min_data_in_leaf': 22, 'feature_fraction': 0.1461947256874997, 'bagging_fraction': 0.320533488858717, 'bagging_freq': 7, 'lambda_l1': 3.150001270501968, 'lambda_l2': 3.4818942772962918, 'num_boost_round': 367}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:29,071] Trial 25 finished with value: -0.09670544271270659 and parameters: {'learning_rate': 0.03646956394875412, 'num_leaves': 58, 'max_depth': 3, 'min_data_in_leaf': 49, 'feature_fraction': 0.1875874863781391, 'bagging_fraction': 0.41881803432858933, 'bagging_freq': 9, 'lambda_l1': 4.055332141249485, 'lambda_l2': 4.418091904108499, 'num_boost_round': 336}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:29,902] Trial 26 finished with value: -0.08360768099529316 and parameters: {'learning_rate': 0.01806602570670552, 'num_leaves': 42, 'max_depth': 4, 'min_data_in_leaf': 38, 'feature_fraction': 0.10114958969509912, 'bagging_fraction': 0.2924519770688999, 'bagging_freq': 3, 'lambda_l1': 2.0820511843544174, 'lambda_l2': 4.96112705921135, 'num_boost_round': 266}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:31,050] Trial 27 finished with value: -0.08893432756216682 and parameters: {'learning_rate': 0.009070048125454741, 'num_leaves': 28, 'max_depth': 2, 'min_data_in_leaf': 14, 'feature_fraction': 0.23459276939435877, 'bagging_fraction': 0.4446430985076796, 'bagging_freq': 5, 'lambda_l1': 4.705235590969872, 'lambda_l2': 2.3575583571826386, 'num_boost_round': 309}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:31,834] Trial 28 finished with value: -0.09761860088544597 and parameters: {'learning_rate': 0.07126179783188274, 'num_leaves': 36, 'max_depth': 3, 'min_data_in_leaf': 19, 'feature_fraction': 0.1351927834669164, 'bagging_fraction': 0.24048965686401713, 'bagging_freq': 2, 'lambda_l1': 3.4374796148214672, 'lambda_l2': 1.7019656868904565, 'num_boost_round': 432}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:33,049] Trial 29 finished with value: -0.11088091091251592 and parameters: {'learning_rate': 0.015108921467206549, 'num_leaves': 59, 'max_depth': 5, 'min_data_in_leaf': 63, 'feature_fraction': 0.30564957957640077, 'bagging_fraction': 0.38485608902728397, 'bagging_freq': 1, 'lambda_l1': 4.122920925282993, 'lambda_l2': 3.087417436634509, 'num_boost_round': 390}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:34,378] Trial 30 finished with value: -0.11251057156073127 and parameters: {'learning_rate': 0.013501400457908657, 'num_leaves': 17, 'max_depth': 6, 'min_data_in_leaf': 55, 'feature_fraction': 0.17642588633165565, 'bagging_fraction': 0.2676004734246992, 'bagging_freq': 6, 'lambda_l1': 2.9967011871266718, 'lambda_l2': 4.0751798835757995, 'num_boost_round': 335}. Best is trial 6 with value: -0.13338677343765942.\n",
      "[I 2025-08-19 15:00:35,435] Trial 31 finished with value: -0.10576204969934325 and parameters: {'learning_rate': 0.024394924848977928, 'num_leaves': 18, 'max_depth': 3, 'min_data_in_leaf': 47, 'feature_fraction': 0.30278012504262897, 'bagging_fraction': 0.20832428350575144, 'bagging_freq': 8, 'lambda_l1': 1.2394131152175338, 'lambda_l2': 1.033372470399265, 'num_boost_round': 372}. Best is trial 6 with value: -0.13338677343765942.\n"
     ]
    }
   ],
   "source": [
    "df_reduced.sort_index(inplace = True)\n",
    "n_train = round(df_reduced.shape[0]*0.8)\n",
    "X_train = df_reduced.iloc[:n_train].drop(columns = ['label'])\n",
    "X_val = df_reduced.iloc[n_train:].drop(columns = ['label'])\n",
    "y_train = df_reduced.iloc[:n_train]['label'] \n",
    "y_val = df_reduced.iloc[n_train:]['label'] \n",
    "\n",
    "ensemble = LightGBMTimeSeriesEnsemble(n_models=15, n_trials=50)\n",
    "ensemble.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7176b377-2ba0-4225-a2f1-13ceff2ee320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ridge alpha: 6000.00000, val corr: 0.12500\n",
      " ridge alpha: 6500.00000, val corr: 0.12505\n",
      " ridge alpha: 7000.00000, val corr: 0.12508\n",
      " ridge alpha: 7500.00000, val corr: 0.12509\n",
      " ridge alpha: 8000.00000, val corr: 0.12510\n",
      " ridge alpha: 8500.00000, val corr: 0.12510\n",
      " ridge alpha: 9000.00000, val corr: 0.12510\n",
      " ridge alpha: 9500.00000, val corr: 0.12510\n",
      " ridge alpha: 10000.00000, val corr: 0.12509\n",
      " ridge alpha: 10500.00000, val corr: 0.12508\n",
      " ridge alpha: 11000.00000, val corr: 0.12507\n",
      " ridge alpha: 11500.00000, val corr: 0.12506\n",
      " ridge alpha: 12000.00000, val corr: 0.12505\n",
      " ridge alpha: 12500.00000, val corr: 0.12503\n",
      " ridge alpha: 13000.00000, val corr: 0.12502\n",
      " ridge alpha: 13500.00000, val corr: 0.12501\n",
      " ridge alpha: 14000.00000, val corr: 0.12500\n",
      " ridge alpha: 14500.00000, val corr: 0.12498\n",
      "Best ridge alpha: 8500.00000, val corr: 0.12510\n"
     ]
    }
   ],
   "source": [
    "# further fine-tuning the ridge regression regularization parameter\n",
    "ensemble.fit_ridge(X_train, y_train, X_val, y_val, range(6000, 15000, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "536abb57-2279-4eaa-918a-a88989bd4888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refitting model 1/20 on full data...\n",
      "Refitting model 2/20 on full data...\n",
      "Refitting model 3/20 on full data...\n",
      "Refitting model 4/20 on full data...\n",
      "Refitting model 5/20 on full data...\n",
      "Refitting model 6/20 on full data...\n",
      "Refitting model 7/20 on full data...\n",
      "Refitting model 8/20 on full data...\n",
      "Refitting model 9/20 on full data...\n",
      "Refitting model 10/20 on full data...\n",
      "Refitting model 11/20 on full data...\n",
      "Refitting model 12/20 on full data...\n",
      "Refitting model 13/20 on full data...\n",
      "Refitting model 14/20 on full data...\n",
      "Refitting model 15/20 on full data...\n",
      "Refitting model 16/20 on full data...\n",
      "Refitting model 17/20 on full data...\n",
      "Refitting model 18/20 on full data...\n",
      "Refitting model 19/20 on full data...\n",
      "Refitting model 20/20 on full data...\n"
     ]
    }
   ],
   "source": [
    "ensemble.refit_full(df_reduced.drop(columns='label'), df_reduced['label'])\n",
    "\n",
    "y_test_pred = ensemble.predict(df_reduced_test)\n",
    "y_test_pred_pd = pd.Series(y_test_pred, name = 'prediction')\n",
    "y_test_pred_pd.index = df_test.index\n",
    "y_test_pred_pd.to_csv('../data/output/submission4.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env310learning)",
   "language": "python",
   "name": "env310learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
